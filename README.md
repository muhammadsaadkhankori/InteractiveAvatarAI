# Digital Being

## Step 1: Creating Avatar
To create an humanoid avatar and most important professor like avatar we use Avaturn [avaturn.me](https://avaturn.me). 

### Steps to create avatar using Avaturn:
1.	First, we uploaded three photos of the face (front, left, and right side).
2.	After uploading the photos, we scrolled down and selected the body type (Male/Female).
3.	We chose avatar type V2 (from the options V1 and V2), as V2 has an animatable face, which is essential.
4.	Click on download to download avatar in (.glb).


## Step 2: Adding Body Animations
To animate the avatar, follow these steps since avatars from different sources (e.g., Avaturn, Ready Player Me) have different skeletons.

### Steps to animate the avatar:
To animate the avatar, we needed to follow several steps since avatars from different sources (like Avaturn or Ready Player Me) have different skeletons.
Steps to animate the body of the avatar:
1.	We converted the downloaded avatar to FBX format using Blender.
2.	We uploaded the FBX avatar to [Mixamo](https://www.mixamo.com), which supports the FBX format, to download various animations (e.g., talking with hands, angry, surprised, etc.). 
3.	Each animation was downloaded individually as a skeleton animation.
4.	We used Blender again to group all these animations into a single file, which we saved as "animations.glb."

## Step 3: Frontend for Avatar Animations
1.	Loads a 3D model and we done changes to dynamically render all meshes such as dress, glasses, caps etc. 
2.	Animates the avatarâ€™s face and body based on user input and predefined expressions.
3.	Maps speech sounds (visemes) to corresponding facial movements for lip-syncing.
4.	Defines facial expressions (e.g., smile, angry, sad) using different facial morph targets.
5.	Uses morph targets to control facial features like eye movement, mouth shapes, and jaw position.
6.	Displays a chat interface where users can interact with the avatar through text or speech.


## Step 4: Backend for Generating Responses
1.	A basic Express server is set up with cors for cross-origin requests. 
2.	Voice API Integration: Uses ElevenLabs API for text-to-speech (TTS) conversion. It converts text input to speech audio files, handling voice stability and speaker boost(elevenLabs).
3.	Speech-to-Text (STS): Utilizes Whisper, a speech recognition model, to convert audio data into text. It processes audio by converting it to MP3 before transcription(whisper)(audios).
4.	Default Messages: If no user input is provided or there are issues (e.g., missing API keys), the system returns pre-set default messages with audio, lip-sync, and facial expressions(defaultMessages).
5.	Lip Sync Functionality: The system generates phonemes from the speech audio using Rhubarb Lip Sync. These phonemes are then used for synchronizing the avatar's lip movements with the generated speech(lip-sync)(rhubarbLipSync).
6.	OpenAI for Text Responses: It uses OpenAI's language model to generate responses based on user questions. The responses include text, facial expressions, and animations for the avatar(openAI).
7.	File Handling: Handles reading and converting audio files into base64 format, executing command-line operations like audio conversion, and reading JSON files for lip-sync data(files).


## Step 5: Dockerization
1.	Dockerize the front-end and back-end together using Docker Compose to run both services in parallel.
2.	In root directory  docker-compose build to build the front-end and back-end, and docker-compose up to run them simultaneously.
3.	To update the avatar, use docker-compose down to stop both services, make changes, and bring them back up with docker-compose up.

